{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4qNTCjKomyAOJ/RQZxAEf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7T-GQQVirez"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayes:\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "        self.vocab = set()\n",
        "        self.class_word_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.class_doc_counts = defaultdict(int)\n",
        "        self.num_docs = 0\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        # Remove punctuations and convert to lowercase\n",
        "        text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "        # Remove stop words\n",
        "        stop_words = set(['a', 'an', 'the', 'in', 'on', 'at', 'of', 'to', 'for', 'by', 'with', 'from', 'and','is'])\n",
        "        tokens = text.split()\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def train(self, documents):\n",
        "        for document, category in documents:\n",
        "            tokens = self.preprocess(document)\n",
        "            self.vocab.update(tokens)\n",
        "            self.class_doc_counts[category] += 1\n",
        "            self.num_docs += 1\n",
        "            for word in tokens:\n",
        "                self.class_word_counts[category][word] += 1\n",
        "\n",
        "    def predict(self, document):\n",
        "        tokens = self.preprocess(document)\n",
        "        posteriors = {category: 0 for category in self.classes}\n",
        "        for category in self.classes:\n",
        "            prior = self.class_doc_counts[category] / self.num_docs\n",
        "            posterior = prior\n",
        "            for word in tokens:\n",
        "                word_count = self.class_word_counts[category][word]\n",
        "                total_count = sum(self.class_word_counts[category].values())\n",
        "                conditional = word_count / total_count\n",
        "                posterior *= conditional\n",
        "            posteriors[category] = posterior\n",
        "        return max(posteriors, key=posteriors.get)\n",
        ""
      ],
      "metadata": {
        "id": "YfQvoH1SlBjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    ('The sky is blue', 'weather'),\n",
        "    ('The sun is bright', 'weather'),\n",
        "    ('The news is depressing', 'politics'),\n",
        "    ('The economy is improving', 'economy'),\n",
        "    ('The movie was great', 'entertainment'),\n",
        "    ('I love pizza', 'food'),\n",
        "    ('The game was exciting', 'sports'),\n",
        "    ('The team played poorly', 'sports'),\n",
        "    ('The election is coming up', 'politics'),\n",
        "]"
      ],
      "metadata": {
        "id": "gvB0b36tlTiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = NaiveBayes(['weather', 'politics', 'economy', 'entertainment', 'food', 'sports'])\n",
        "nb.train(docs)\n",
        "\n",
        "# Predict the category of a new document\n",
        "new_doc1 =  'pizza love'\n",
        "new_doc2 = 'election'\n",
        "new_doc3 = 'She likes rainy weather'\n",
        "category1 = nb.predict(new_doc1)\n",
        "category2 = nb.predict(new_doc2)\n",
        "category3 = nb.predict(new_doc3)\n",
        "print(f'The document \"{new_doc1}\" belongs to the category \"{category1}\"')\n",
        "print(f'The document \"{new_doc2}\" belongs to the category \"{category2}\"')\n",
        "print(f'The document \"{new_doc3}\" belongs to the category \"{category3}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKpOubpplXvo",
        "outputId": "2b221b49-32ab-4b80-dc10-672eb016b80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document \"pizza love\" belongs to the category \"food\"\n",
            "The document \"election\" belongs to the category \"politics\"\n",
            "The document \"She likes rainy weather\" belongs to the category \"weather\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [category1,category2,category3]\n",
        "y_true = ['food','politics','weather']\n",
        "cm = confusion_matrix(y_true,y_pred)\n",
        "print(cm)\n",
        "\n",
        "acc_score = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy Score: {acc_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Fx4COGqn5z",
        "outputId": "de96c5d5-e1ec-4652-b7a1-1287c0f7b672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]]\n",
            "Accuracy Score: 1.00\n"
          ]
        }
      ]
    }
  ]
}